机器学习
一、机器学习概述
根据经验自我完善
有监督学习：根据已知的结果对机器给出的答案进行修正。
无监督学习：在未知正确结果的前提下由机器来发现规则。
半监督学习：先通过无监督学习挖掘隐藏在数据背后的某种模式，然后再通过有监督学习的过程验证该模式的正确性，并据此决定下一步对学习过程的调整。
强化学习：由智能体自主地选择决策方向，当决策接近正确时会得到奖励，反之会受到惩罚，只要经历足够长的时间，智能体所做出决策在大概率上会越来越正确。
训练：发现规则，总结规律。
评价：验证规则，实践规律。
修正：修改规则，更新规律。
批量学习：在一个固定的输入集上完成训练、评价和修正。
增量学习：只在一部分输入集上完成训练、评价和修正，分多次迭代，直到学习系统的性能满足预期。
基于实例的学习：经验主义哲学。
基于模式的学习：理性主义哲学。
机器学习的基本问题：
回归问题：根据输入预测连续域内的输出。
输入   输出
1        2
2        4
3        6
4        8
5        ? -> 10
分类问题：根据输入预测离散域内的输出。
输入                      输出
25/男/大本/2 -> 中等收入
26/女/大专/3 -> 中等收入
35/男/硕士/5 -> 高等收入
...
聚类问题：基于样本的相似程度将其划分为不同的族群。
降维问题：减少输入数据的特征数，简化模型。
机器学习的一般过程：
数据采集
数据清洗
数据预处理
选择模型(算法) y = kx + b
训练模型(规则) k/b -> 模型参数
测试模型(验证) 测试效果不佳返回训练，甚至更换模型，直到满足预期
应用模型(业务)
维护模型(升级)
机器学习系统不是盲目的系统，而是有预期，有理念的对数据的洞见。
二、机器学习框架
scikit-learn(sklearn)：经典机器学习框架
tensorflow：深度(神经网络)学习框架
tflearn：基于tensorflow的高级封装
样本矩阵：二维数组，一行一样本，一列一特征
张三,20,男,大本,北京,一年 -> 5000
李四,22,男,大本,西安,两年 -> 6000
...
输入数据(50,6)->(实际)输出数据(50,)
xxx.fit(输入数据, 输出数据) # 训练
xxx.predict(输入数据) -> (预测)输出数据 # 预测
通过不同的指标表示预测输出和实际输出之间的匹配程度。
[0,1]
差,好
三、数据预处理
1.标准化(均值移除)
将样本矩阵中的各个特征(列)的算数平均值调整为0，标准差调整为1，以使不同特征对模型预测结果的贡献接近相等。
a b c
m=(a+b+c)/3
a' b' c'
a'=a-m
b'=b-m
c'=c-m
m'=(a'+b'+c')/3=[(a+b+c)-3m]/3=m-m=0
s=sqrt{[(a-m)^2+(b-m)^2+(c-m)^2]/3}
  =sqrt{(a'^2+b'^2+c'^2)/3}
a"=a'/s
b"=b'/s
c"=c'/s
s"=sqrt{(a"^2+b"^2+c"^2)/3}
    =sqrt((a'^2+b'^2+c'^2)/(3s^2)}
    =sqrt{(3s^2)/(3s^2)}=1
import sklearn.preprocessing as sp
sp.scale(原始样本矩阵) -> 均值移除后的样本矩阵
代码：std.py
2.范围缩放
对样本矩阵中的各个列做线性缩放，使其最小值和最大值等于某个预先给定的值。
x'=kx+b
min'=kmin+b
max'=kmax+b
[0,1]
/ min 1 \  x / k \ = / min' \
\ max 1 /    \ b /     \ max' /
----------     -----    ---------
      A              x             b
                       = numpy.linalg.solve(A, b)
mms = sp.MinMaxScaler(feature_range=(0, 1))
mms.fit_transform(原始样本矩阵)->范围缩放后的样本矩阵
fit：训练，建立模型，计算参数，得到k和b
transform：转换，利用参数对输入数据做变换，返回结果
代码：mms.py
平均值为0标准差为1的标准化预处理    \  "归一化"预处理
最小值为0最大值为1的范围缩放预处理 /
3.归一化
用每个样本的每个特征值除以该样本的L1范数。
L1范数，即一个样本各个特征值的绝对值之和。
用特征值的占比表示特征的价值。
           Python    C++    Java    PHP
2017  20            50        40       10
2018  10            10        20       0
           1/6          5/12    1/3      1/12 -> SIGMA=1      
           1/4
L2范数，即一个样本各个特征值的绝对值平方之和。
sp.normalize(原始样本矩阵, norm='l1')
    ->归一化后的样本矩阵                    ^--> L1范数
代码：nor.py
4.二值化
首先设定一个阈值，将样本矩阵中的每个特征值与该阈值做比较，大于阈值的特征值用1表示，其它特征值用0表示。
bin = sp.Binarizer(threshold=阈值)
bin.transform(原始样本矩阵)->二值化后的样本矩阵
代码：bin.py
5.独热编码
1      3      2
7      5      4
1      8      6
7      3      9
---------------------
1:10 3:100 2:1000
7:01 5:010 4:0100
         8:001 6:0010
                    9:0001
---------------------
101001000
010100100
100010010
011000001
ohe = sp.OneHotEncoder(
    sparse=是否采用稀疏矩阵形式，默认True,
    dtype=元素类型)
ohe.fit_transform(原始样本矩阵)->独热编码后的样本矩阵
代码：ohe.py
6.标签编码
将文本形式的特征值编码为数字。
将样本矩阵中的文本特征按照字典顺序做升序排列，用每个特征值的顺序号代替该值。
... apple         ...
... pineapple ...
... orange      ...
... banana     ...
apple banana orange pineapple
    0          1           2            3
... 0 ...
... 3 ...
... 2 ...
... 1 ...
lbe = sp.LabelEncoder()
lbe.fit_transform(原始特征向量(样本矩阵中的一列))
    ->标签编码后的特征向量
lbe.inverse_transform(标签编码后的特征向量)
    ->原始特征向量
代码：lab.py
四、梯度下降算法
y = f(x)
y' = f'(x)
f'(x) = 0
z = f(x, y)
一元线性回归问题
输入    输出
0.5     5.0
0.6     5.5
0.8     6.0
1.1     6.8
1.4     7.0
x        y
y = w0+w1x，预测函数
找到足够好的w0和w1，即最优模型参数，满足已知输入和输出的对应关系，即达到最小误差。
SIGMA((y-(w0+w1x))^2)
----------------------------- = loss，总样本误差
                     2
loss = f(w0, w1)，损失函数
找到足够理想的模型参数w0和w1，使loss极小。
                        SIGMA(-2(y-(w0+w1x)))
Dloss/Dw0 = -----------------------------
                                         2
                    = - SIGMA(y-(w0+w1x))
                        SIGMA(-2(y-(w0+w1x))x)
Dloss/Dw1 = -----------------------------
                                          2
                    = SIGMA((y-(w0+w1x))x)
w0 = w0 - r Dloss/Dw0
w1 = w1 - r Dloss/Dw1
          w0 w0 w0 ... w0
          w1 w1 w1 ... w1
  y   x  y'   y'   y'   ... y' 
  y   x  y'   y'   y'   ... y'
  y   x  y'   y'   y'   ... y'
     ...  ...
  y   x  y'   y'   y'   ... y'
           l     l     l     ... l
代码：bgd.py
五、线性回归
import sklearn.linear_model as lm
model = lm.LinearRegression() # 线性回归器
model.fit(输入矩阵，输出向量) # 训练，用GD算法找w0...wn
model.predict(输入矩阵)->输出向量
y = w0+w1x1+w2x2+...+wnxn
代码：linear.py
        1
r2 -----
    1+e
e: [0, oo)
      1  0
import pickle
# 模型序列化
with open(文件路径, 'wb') as f:
    pickle.dump(model, f)
    # 内存中的模型对象->磁盘中的模型文件
# 模型反序列化
with open(文件路径, 'rb') as f:
    model = pickle.load(f)
    # 磁盘中的模型文件-> 内存中的模型对象
代码：dump.py、load.py
六、岭回归
标准的线性回归模型，对所有样本一视同仁地对待，根据所有训练样本的误差计算其算数平均值表示损失值。因此，少数明显偏离的样本会对模型产生较大的负面影响，即使得模型更倾向于少数异常样本，反而降低了其对正常样本的拟合度。
岭回归在线性回归的基础上，根据样本的统计直方图，为分布于不同密度区间的训练样本分配不同的权重，越密集的分布区间权重越高，相反越稀疏的分布区间权重越低，这样在计算损失值时考虑不同样本误差的加权平均，以此降低少数异常样本对模型的影响。
model = lm.Ridge(正则强度,
    fit_intercept=调整截距, max_iter=最大迭代次数)
正则强度
代码：rdg.py
七、多项式回归
x -> y
y = w0 + w1x
y = w0 + w1x + w2x^2 + ... + wnx^n  一元n次
y = w0 + w1x1 + w2x2 + ... + wnxn     n元一次
x->多项式特征扩展器-x1...xn->线性回归器->w0...wn
      \_____________________________________/
                                   管线
多项式特征扩展器 = sp.PolynomialFeatures(n)
线性回归器 = lm.LinearRegression()
import sklearn.pipeline as pl
管线 = pl.make_pipeline(多项式特征扩展器, 线性回归器)
管线.fit(x, y) # w0...wn
管线.predict(x) -> y
代码：poly.py
模型与训练及测试样本的匹配程度：
欠拟合：模型对训练样本和测试样本均表现为较差的拟合效果。
过拟合：模型对训练样本匹配度极高，但是对测试样本却表现为较差的匹配度。
理想拟合：模型对训练样本和测试样本都能够表现出较好的拟合效果。
                                  R2得分
训练集      0.4        0.9                     0.8
测试集      0.3        0.5                     0.7
              欠拟合   过拟合               理想拟合
           增加样本  降低模型复杂度
增加模型复杂度  增大正则强度
八、决策树
年龄：1-青年/2-中年/3-老年
学历：1-大专/2-本科/3-硕士以上
院校：1-普通/2-985/3-211
经验：1-小白/2-缺少/3-丰富/4-资深
薪资：1-低/2-中/3-高
年龄 学历 院校 经验 -> 薪资
  1      2      1      1          低  5000
  2      1      1      3          中  8000
  1      3      3      2          高  20000
  3      3      2      4          中  6000
  ...
  2      1      1      3   ->   ?
分类：投票
回归：平均
相似的输入->相似的输出
                                                   根表
年龄:            子表1                     子表2                 子表3
学历: 学历1 学历2 学历3 学历1 学历2 学历3          ...
院校:
经验:
选择部分重要特征减少层次。
每次划分子表选择使信息熵减少量最大的特征。
每次划分子表选择使基尼不纯度减少量最大的特征。
通过集成算法避免预测偏斜：按照不同的原则，构造多棵决策树，利用投票或平均综合它们的预测结果。
自助聚合：从总样本空间中随机抽取部分样本构建决策树，用体现不同样本区间的多棵树均化不同影响力的样本对决策结果的影响。
随机森林：在自助聚合的基础上，每次不但随机抽取部分样本，而且每次构建决策树时所依据的特征集也是随机抽取的。
正向激励：为样本矩阵中的每个样本分配初始权重，构建第一棵决策树，用该决策数对训练集做预测，将预测错误的样本的权重提高一定比例，再构建第二棵决策树，如此重复，得到B棵样本权重分布不同的决策树。
import sklearn.tree as st
st.DecisionTreeRegressor(max_depth=最大树高)
单棵决策树回归器
import sklearn.ensemble as se
se.AdaBoostRegressor(
    决策树回归器, n_estimators=决策树棵数,
    random_state=随机种子)
正向激励决策树集合
import sklearn.datasets as sd
波士顿地区房价数据集
代码：house.py
特征重要性：决策树模型再根据信息熵减少量选择特征时，会为每个特征计算它的重要程度，即对输出构成影响的程度。
model.feature_importances_
超参数：人为事先给定，比如多项式次数，正则强度等
模型参数：通过训练找到的组成模型的最优参数，如w0、w1等
学习参数：在学习的过程中额外产生一些有价值的数据
代码：fi.py
不同的模型计算信息熵的依据会有差异，因此所得到的特征重要性不会完全一致，但大体相当。相同的模型，训练数据的粒度不同，也会导致特征重要性的差异。无论哪种情况，有关特征重要性的计算皆源自对数据的统计，因此总可以构成对业务分析的指导，以获得近似一致的商业信息。
代码：bike.py












